ğŸš€ Pipeline de Dados IBGE: Localidades Brasileiras com Airflow & Docker

Este projeto demonstra a implementaÃ§Ã£o de um pipeline de dados ETL (Extract, Transform, Load) orquestrado, utilizando Apache Airflow rodando em containers Docker. O objetivo Ã© extrair informaÃ§Ãµes de estados e regiÃµes brasileiras diretamente da API de Localidades do IBGE, processÃ¡-las e armazenÃ¡-las de forma organizada para anÃ¡lise.

ğŸ› ï¸ Tecnologias Utilizadas

â€¢Linguagem: Python 

â€¢OrquestraÃ§Ã£o: Apache Airflow 

â€¢Processamento de Dados: Pandas

â€¢Infraestrutura: Docker & Docker Compose


ğŸ“‹ Arquitetura do Projeto

O pipeline foi desenhado seguindo as melhores prÃ¡ticas de engenharia de dados, separando a lÃ³gica de negÃ³cio da orquestraÃ§Ã£o:

â€¢ExtraÃ§Ã£o: Consumo da API REST do IBGE utilizando a biblioteca requests.

â€¢TransformaÃ§Ã£o: Limpeza e normalizaÃ§Ã£o de dados JSON complexos em DataFrames estruturados, separando entidades de UFs e RegiÃµes.

â€¢Carga: PersistÃªncia dos dados transformados em arquivos CSV em um volume compartilhado entre o container e a mÃ¡quina local.

ğŸ“‚ Estrutura de Pastas

.
â”œâ”€â”€ dags/               # DefiniÃ§Ã£o da DAG do Airflow
â”œâ”€â”€ pipelines/          # Scripts Python com a lÃ³gica de ETL
â”œâ”€â”€ dados/              # Pasta local onde os CSVs sÃ£o salvos (Volume Docker)
â”œâ”€â”€ docker-compose.yaml # ConfiguraÃ§Ã£o da infraestrutura Airflow
â”œâ”€â”€ .env.example        # Modelo de variÃ¡veis de ambiente
â””â”€â”€ requirements.txt    # DependÃªncias do projeto



ğŸš€ Como Executar

PrÃ©-requisitos

â€¢Docker Desktop instalado

â€¢Git

Passo a Passo

1.Clone o repositÃ³rio:

git clone https://github.com/seu-usuario/ibge-localidades-airflow.git
cd ibge-localidades-airflow


2.Configure as variÃ¡veis de ambiente:
Crie um arquivo .env na raiz do projeto com base no .env.example:


PATH_DADOS=/opt/airflow/dados
AIRFLOW_UID=50000


3.Inicie o ambiente Airflow:
No terminal (Windows/CMD ), execute:

set AIRFLOW_UID=50000
docker-compose up -d


4.Acesse a interface do Airflow:
Abra o navegador em http://localhost:8080

â€¢User: airflow | Password: airflow

5.Execute a DAG:

Ative a DAG IBGE_Localidades e dispare o gatilho (Trigger). Os arquivos serÃ£o gerados na pasta dados/ do seu computador.

ğŸ§  Desafios Superados

Durante o desenvolvimento, foram aplicadas soluÃ§Ãµes para:

â€¢ConfiguraÃ§Ã£o de PYTHONPATH: Garantir que o Airflow localize mÃ³dulos Python customizados dentro de containers.

â€¢Gerenciamento de Volumes: SincronizaÃ§Ã£o em tempo real entre o sistema de arquivos do container (Linux) e o host (Windows).

â€¢InstalaÃ§Ã£o DinÃ¢mica de DependÃªncias: Uso de _PIP_ADDITIONAL_REQUIREMENTS para manter a imagem leve e funcional.

